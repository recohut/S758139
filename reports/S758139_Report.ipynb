{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S758139_Report",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RecoHut-Stanzas/S758139/blob/main/reports/S758139_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SilLoGOVq8Sz"
      },
      "source": [
        "# DRGR: Deep Reinforcement learning based Group Recommender system\n",
        "\n",
        "## Summary Table\n",
        "| Category | Description |\n",
        "| --- | --- |\n",
        "| Problem | Group recommendation problem is challenging because recommending some item that satisfies all the members of the group is rare. So it often involves some compromises that the model has to make in order to maximize the overall satisfaction of the group. |\n",
        "| Hypothesis | RL agent can learn the required behavior that could the maximize the group's overall satisfaction. |\n",
        "| Benefits | Meaningful to those people who want to get recommendations for their groups, such as entertainments with families and travels with friends. This model consider the influences of each group member by one self-attention mechanism. |\n",
        "| Solution | A recommender agent is trained with actor-critic network and is optimized with DDPG algorithm, where the experience replay and target networks are used. Matrix factorization based simulator is built to simulate the MDP environment. It is an extended version of LIRD model for group recommendations. The group recommendation is viewed as a classification task. When one item is recommended to a group, if the group chooses the item, this case is marked as a positive sample. Otherwise, it will be a negative sample. |\n",
        "| Dataset | MovieLens-1m |\n",
        "| Preprocessing | Randomly generate groups with 2-5 users. Then, for each group, if every member gives 4-5 stars to one movie, we assume that this movie is adopted by this group with rating 1. If all members give ratings to one movie, but not all in 4-5 stars, we consider the group gives rating 0 to this movie. For other cases, the group movie ratings are missed. Finally, to ensure each group has enough interactions with items, we require each group has at least 20 ratings. Also, for each rating, 100 rating-missed items are randomly sampled. Both user and group rating data are split into training, validation, and testing datasets with the ratio of 70%, 10%, and 20% respectively by the temporal order. |\n",
        "| Metrics | Recall, nDCG |\n",
        "| Cluster | PyTorch |\n",
        "| Tags | ActorCriticNetwork, DDPG, GroupRecommendation, MovieLens1M, PyTorch, ReplayMemory |\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "The recommender system is viewed as an agent, which interacts with the environment $\\mathcal{E}$ by recommending items to maximize its reward. The environment is a set of multiple groups, where one single user is assumed to be one member group here. This task is modeled as a Markov Decision Process (MDP). Formally, this MDP contains five elements $\\mathcal{(S, A, P, R,\\gamma)}$ as follows. Let $\\mathcal{U}$, $\\mathcal{G}$, and $\\mathcal{I}$ be sets of user, group, and item ids respectively.\n",
        "\n",
        "- State space $\\mathcal{S}$: A state $ğ‘ _ğ‘¡ \\in \\mathcal{S}$ represents the state of a group at time ğ‘¡. A state $ğ‘ _ğ‘¡ = [ğ‘”, â„_ğ‘¡]$ consists of two parts, one for the group id ğ‘” âˆˆ G and another for the group browsing history $â„_ğ‘¡$. The group id ğ‘” can be mapped to its members, i.e. ğ‘” = {ğ‘¢1, ğ‘¢2, . . . }, where ğ‘¢1, ğ‘¢2, Â· Â· Â· âˆˆ U. The browsing history of a group is â„ğ‘¡ = [ğ‘–1,ğ‘–2, . . . ,ğ‘–ğ‘ ], where ğ‘–1,ğ‘–2, . . . ,ğ‘–ğ‘ âˆˆ $\\mathcal{I}$.\n",
        "- Action space $\\mathcal{A}$: An action ğ‘ğ‘¡ = [ğ‘–ğ‘¡,1,ğ‘–ğ‘¡,2, . . . ,ğ‘–ğ‘¡,ğ¾] âˆˆ $\\mathcal{A}$ is a list of items recommended to a group from the recommender system, where ğ¾ is the number of items. For learning efficiency, we will use ğ¾ = 1 when the agent interacts with the environment, i.e. ğ‘ğ‘¡ âˆˆ $\\mathcal{I}$.\n",
        "- Reward $\\mathcal{R}$: After an action ğ‘ğ‘¡ is taken by the recommender to a group at one state ğ‘ ğ‘¡, the recommender will receive a reward ğ‘Ÿğ‘¡ âˆˆ {0, 1} based on the group response. The reward will be ğ‘Ÿğ‘¡ = 1, if the recommended item is picked by this group. Otherwise, ğ‘Ÿğ‘¡ = 0.\n",
        "- Transition probability $\\mathcal{P}$: Transition probability is defined as $ğ‘(ğ‘ _{ğ‘¡+1} |ğ‘ _ğ‘¡,ğ‘_ğ‘¡)$, which satisfies the Markov property. This probability measures how the environment evolve with the time ğ‘¡.\n",
        "- Discount factor ğ›¾: The ğ›¾ âˆˆ [0, 1] measures how the future reward will be valuated today.\n",
        "\n",
        "By given this MDP $\\mathcal{(S, A, P, R,\\gamma)}$, the agent will try to maximize its rewards by taking actions to interact with the environment. The solution will be the policy ğœ‹ : $\\mathcal{S}$ â†’ $\\mathcal{A}$.\n",
        "\n",
        "## Modules\n",
        "\n",
        "### Environment Simulator\n",
        "<p><center><figure><img src='https://github.com/RecoHut-Stanzas/S758139/raw/main/images/env_sim.png'><figcaption><i>The framework of the environment, where the matrix factorization is used to simulate unknown rewards.</i></figcaption></figure></center></p>\n",
        "\n",
        "$$h_{t+1} = \\begin{cases} [i_2,\\dots,i_N,a_t], &\\quad r_t \\gt0 \\\\ h_t, &\\quad r_t \\le 0\\end{cases}$$\n",
        "\n",
        "### Agent Framework\n",
        "<p><center><figure><img src='https://github.com/RecoHut-Stanzas/S758139/raw/main/images/agent.png'><figcaption><i>The framework of the agent, where state embedding layer, actor network, and critic network are included.</i></figcaption></figure></center></p>\n",
        "\n",
        "In order to solve the MDP, actor-critic model is built. This agent model consists three networks, i.e. state embedding network, actor network, and critic network.\n",
        "\n",
        "### State Embedding\n",
        "\n",
        "The state embedding network is to embed the state $ğ‘ _ğ‘¡ = [ğ‘”, â„_ğ‘¡]$ to its embedding $s_ğ‘¡$. The group id ğ‘” is mapped to its group members  $ğ‘” = \\{ğ‘¢_1, ğ‘¢_2, \\dots\\}$ first, then those user ids are embedded through the user embedding layer to $\\{u_1, u_2, \\dots\\}$. Then a self-attention user aggregation layer is used to get the embedded group $g$.\n",
        "\n",
        "Meanwhile, the history $â„_ğ‘¡ = [ğ‘–_1,ğ‘–_2, \\dots,ğ‘–_ğ‘]$ is embedded through the item embedding layer to the embedded history. Combining these two parts, we get the embedded state $s_ğ‘¡ = [g, h_ğ‘¡]$.\n",
        "\n",
        "### Actor Network\n",
        "\n",
        "The actor is a multi-layer neural network, whose input is the embedded state $s_ğ‘¡$ and output is the action weight $w_ğ‘¡$. To promote exploration, the Ornsteinâ€“Uhlenbeck process noise can be added here. This action weight will take inner products with item embeddings, i.e. $ğ‘ _ğ‘— = w_ğ‘¡^T i_ğ‘—$. The item with the highest $ğ‘ _ğ‘—$ will be recommended to the group, and the corresponding item embedding will be sent to the critic next. The actor network can be optimized by the policy gradient.\n",
        "\n",
        "### Critic Network\n",
        "\n",
        "The critic is also a multi-layer neural network, which will assign the Q-value of one state-action pair, i.e. $ğ‘„(s_ğ‘¡ , a_ğ‘¡)$. The critic network can be optimized by the temporal difference method as the Deep Q-Learning.\n",
        "\n",
        "## Tutorials\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RecoHut-Stanzas/S758139/)\n",
        "\n",
        "### Converting MovieLens-1m data into Group dataset\n",
        "\n",
        "[direct link to notebook â†’](https://github.com/RecoHut-Stanzas/S758139/blob/main/nbs/T277604_Group_Data_Generator_on_ML_1m.ipynb)\n",
        "\n",
        "Randomly generate groups with 2-5 users. Then, for each group, if every member gives 4-5 stars to one movie, we assume that this movie is adopted by this group with rating 1. If all members give ratings to one movie, but not all in 4-5 stars, we consider the group gives rating 0 to this movie. For other cases, the group movie ratings are missed. Finally, to ensure each group has enough interactions with items, we require each group has at least 20 ratings. Also, for each rating, 100 rating-missed items are randomly sampled. Both user and group rating data are split into training, validation, and testing datasets with the ratio of 70%, 10%, and 20% respectively by the temporal order.\n",
        "\n",
        "**Input files:**\n",
        "\n",
        "Raw ML-1m dataset downloaded from [https://files.grouplens.org/datasets/movielens/ml-1m.zip](https://files.grouplens.org/datasets/movielens/ml-1m.zip).\n",
        "\n",
        "**Output files:**\n",
        "\n",
        "| | |\n",
        "| --- | --- |\n",
        "| movies.dat | Movie information file from MovieLens-1M |\n",
        "| users.dat | User information file from MovieLens-1M |\n",
        "| groupMember.dat | File including group members. Each line is a group instance: groupID userID1,userID2,... |\n",
        "| group(user)RatingTrain.dat | Train file. Each line is a training instance: groupID(userID) itemID rating timestamp |\n",
        "| group(user)RatingVal(Test).dat | group (user) validation (test) file (positive instances). Each line is a validation (test) instance: groupID(userID) itemID rating timestamp |\n",
        "| group(user)RatingVal(Test)Negative.dat | group (user) validation (test) file (negative instances). Each line corresponds to the line of group(user)RatingVal(Test).dat, containing 100 negative samples. Each line is in the format: (groupID(userID),itemID) negativeItemID1, negativeItemID2, ... |\n",
        "\n",
        "Interestingly, this can be used as a generic module also to convert user-item dataset into group dataset suitable to train group recommenders. e.g. we can prepare the ML-1m dataset with these 2 commands:\n",
        "\n",
        "```python\n",
        "# https://gist.github.com/sparsh-ai/37b36b4024a8e289bbabfcae8ca24bfa/7aad5dca0b028e563a874cce6af2b30f356c8113\n",
        "!wget -q --show-progress ml1m_groupdata_preprocess.ipynb https://gist.githubusercontent.com/sparsh-ai/37b36b4024a8e289bbabfcae8ca24bfa/raw/7aad5dca0b028e563a874cce6af2b30f356c8113/t277604-group-data-generator-on-ml-1m.ipynb\n",
        "%run ml1m_groupdata_preprocess.ipynb\n",
        "```\n",
        "\n",
        "### DDPG algorithm on the Inverted Pendulum Problem in Keras\n",
        "\n",
        "[direct link to notebook â†’](https://github.com/RecoHut-Stanzas/S758139/blob/main/nbs/T085517_Implementing_DDPG_algorithm_on_the_Inverted_Pendulum_Problem_in_Keras.ipynb)\n",
        "\n",
        "This is an optional tutorial to get us familiarize with the DDPG algorithm. This is taken from the Keras official tutorial collection. In this tutorial, we are solving the inverted pendulum problem using DDPG algorithm.\n",
        "\n",
        "### Group Recommendation with Actor-Critic Network using DDPG algorithm on ML-1m\n",
        "\n",
        "[direct link to notebook â†’](https://github.com/RecoHut-Stanzas/S758139/blob/main/nbs/T381683_Group_Recommendations_with_Actor_critic_RL_Agent_in_MDP_Environment_on_ML_1m_Dataset.ipynb)\n",
        "\n",
        "## Supplementary Material\n",
        "\n",
        "### Deep Reinforcement Learning in Recommendation Systems\n",
        "\n",
        "[https://sparsh-ai.github.io/drl-recsys](https://sparsh-ai.github.io/drl-recsys/R984600_DRL_in_RecSys.html) is a project dedicated to this topic.\n",
        "\n",
        "### Actor-Critic Network\n",
        "\n",
        "The actor-network is basically the policy network, and it finds the optimal policy using a policy gradient method. The critic network is the value network, and it estimates the state value.\n",
        "\n",
        "<img src='https://github.com/RecoHut-Stanzas/S758139/raw/main/images/actorcritic.png'>\n",
        "\n",
        "The fundamental difference between the REINFORCE and the AC method is that in the REINFORCE, we update the parameter of the network at the end of an episode. But in the AC method, we update the parameter of the network at every step of the episode. \n",
        "\n",
        "### DDPG\n",
        "\n",
        "DDPG, or Deep Deterministic Policy Gradient, is an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. It combines the actor-critic approach with insights from DQNs: in particular, the insights that 1) the network is trained off-policy with samples from a replay buffer to minimize correlations between samples, and 2) the network is trained with a target Q network to give consistent targets during temporal difference backups. DDPG makes use of the same ideas along with batch normalization.\n",
        "\n",
        "It combines ideas from DPG (Deterministic Policy Gradient) and DQN (Deep Q-Network). It uses Experience Replay and slow-learning target networks from DQN, and it is based on DPG, which can operate over continuous action spaces.\n",
        "\n",
        "<img src='https://github.com/RecoHut-Stanzas/S758139/raw/main/images/ddpg_algo.png'>\n",
        "\n",
        "### Experience Replay\n",
        "\n",
        "We couldn't just blindly apply RL algorithms in a production system out of the box. The learning period would be too costly. Instead, we need to leverage the vast amounts of offline training examples to make the algorithm perform as well as the current system before releasing it into the online production environment.\n",
        "\n",
        "Both bandit algorithms and A/B tests are online algorithms, which means that they do not have a full set of data upfront to be trained on. Instead, they learn incrementally as the data is accrued. Their performance can be evaluated offline, however, through backtesting using a technique known as the â€œreplayâ€ method. This works by predicting which option the algorithm would select for a viewer, and if there is a historical record of the viewerâ€™s interaction with that option, the result â€” whether the experience was positive or negative â€” is counted as if it had happened. If there is no historical record of that interaction, it is ignored.\n",
        "\n",
        "> **Bandits provide the most benefit when there is a cost associated with making a suboptimal suggestion.**\n",
        "> \n",
        "\n",
        "Banditâ€™s recommendations will be different from those generated by the model whose recommendations are reflected in your historic dataset. This creates problems that lead to some of the key challenges in evaluating these algorithms using historic data.\n",
        "\n",
        "1. this is problematic is that your data is probably biased\n",
        "2. algorithm will often produce recommendations that are different from the recommendations seen by users in the historic dataset\n",
        "\n",
        "The solution to the above problem is calledÂ **replay**. Replay evaluation essentially takes historic events and algorithmâ€™s recommendations at each time step and throws out all samples except for those where your modelâ€™s recommendation is the same as the one the user saw in the historic dataset.\n",
        "\n",
        "**Toy implementation of Replay**\n",
        "\n",
        "```python\n",
        "class ReplayMemory(object):\n",
        "    \"\"\"\n",
        "    Replay Memory\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, buffer_size: int):\n",
        "        \"\"\"\n",
        "        Initialize ReplayMemory\n",
        "        :param buffer_size: size of the buffer\n",
        "        \"\"\"\n",
        "        self.buffer_size = buffer_size\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def push(self, experience: tuple):\n",
        "        \"\"\"\n",
        "        Push one experience into the buffer\n",
        "        :param experience: (state, action, reward, new_state)\n",
        "        \"\"\"\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        \"\"\"\n",
        "        Sample one batch from the buffer\n",
        "        :param batch_size: number of experiences in the batch\n",
        "        :return: batch\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        return batch\n",
        "```\n",
        "\n",
        "## Links & References\n",
        "\n",
        "1. [\"DRGR: Deep Reinforcement Learning based Group Recommender System\" by Zefang Liu, Shuran Wen, and Yinzhu Quan. arXiv, 2021](https://arxiv.org/abs/2106.06900v1) `paper`\n",
        "2. [https://github.com/zefang-liu/group-recommender](https://github.com/zefang-liu/group-recommender) `code`\n",
        "3. [https://github.com/sparsh-ai/stanza/tree/S758139](https://github.com/sparsh-ai/stanza/tree/S758139) `code`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiYlCHazzxCG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}